The typology of sound systems in spoken human languages should be explained in part by functional pressures on communication.  Two competing pressures are transmission rate and ease of communication.  More information may be transmitted per phoneme token if more phonemes are available---but fitting more phonemes into the system would require the system to use outlier sounds that are hard to pronounce or perceive, or else to split existing phonemes, requiring more speaker or hearer effort to keep them distinct.  In contrast, a system with few phonemes has limited information per phoneme, but speakers can articulate the phonemes more easily and sloppily (perhaps allowing more phonemes per second). We encode these two competing pressures into a proposed universal prior for a generative probability model. We find that a model of vowel token formants is more predictive of held-out data if it is trained with the help of this prior (that is, by MAP rather than ML).