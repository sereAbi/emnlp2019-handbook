Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning.
In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus.