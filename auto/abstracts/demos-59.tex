The main alternatives nowadays to deal with sequences are Recurrent Neural Networks (RNN) architectures and the Transformer. 
In this context, Both RNN's and Transformer have been used as an encoder-decoder architecture with multiple layers in each module. Far beyond this, these architectures are the basis for the contextual word embeddings which are revolutionizing most natural language downstream applications.