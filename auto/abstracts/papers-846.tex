In this study, we  first investigate a novel capsule network
with dynamic routing for linear time Neural  Machine Translation (NMT), referred as
\textsc{CapsNMT}.
\textsc{CapsNMT}
uses an aggregation mechanism to map the source sentence into a matrix with pre-determined
size, and then applys a deep LSTM network to decode the
target sequence from the source representation.
Unlike the previous
work \cite{sutskever2014sequence} to store
the  source sentence with a passive and bottom-up way,
the dynamic routing policy encodes the source sentence with
an iterative process to
decide the credit attribution between nodes from lower and higher layers.
\textsc{CapsNMT} has two core properties:
it runs in time that is linear in the length of the sequences
and
provides a more flexible way to aggregate the part-whole information of the source
sentence.
On WMT14 English-German task and a larger WMT14 English-French task, \textsc{CapsNMT} achieves comparable results
with the Transformer system.
To the best of our knowledge, this is the first work that capsule networks have been empirically investigated
for  sequence to sequence problems.