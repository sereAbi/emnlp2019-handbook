Entity resolution for noisy ASR transcripts 
Arushi Raghuvanshi, Vijay Ramakrishnan, Varsha Embar, Lucien Carroll, and Karthik Raghunathan 

Incorporating External Knowledge into Machine Reading for Generative Question An- swering
Bin Bi, Chen Wu, Ming Yan, Wei Wang, Jiangnan Xia, and Chenliang Li
Commonsense and background knowledge is required for a QA model to answer many nontrivial ques- tions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with con- text. 

Semantic graph parsing with recurrent neural network DAG grammars 
Federico Fancellu, Sorcha Gilroy, Adam Lopez, and Mirella Lapata 14:24–14:42 Semantic parses are directed acyclic graphs (DAGs), so semantic parsing should be modeled as graph prediction. But predicting graphs presents difficult technical challenges, so it is simpler and more common to predict the 


Projection Sequence Networks for On-Device Text Classification 
Zornitsa Kozareva and Sujith Ravi 17:06–17:24 We propose a novel on-device sequence model for text classification using recurrent projections. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks. 

Implicit Deep Latent Variable Models for Text Generation 
Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen 
Deep latent variable models (LVM) such as variational auto-encoder (VAE) have recently played an im- portant role in text generation. One key factor is the exploitation of smooth latent structures to guide the generation. However, the representation power of VAEs is limited due to two reasons: (1) the Gaussian assumption is often made on the variational posteriors; and meanwhile (2) a notorious 

TaskMaster Dialog Corpus: Toward a Realistic and Diverse Dataset 
Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik 10:30–10:48 
A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six do- mains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken 

Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network 
Shuqing Bian, Wayne Xin Zhao, Yang Song, Tao Zhang, and Ji-Rong Wen 
Person-job fit has been an important task which aims to automatically match job positions with suitable candidates. Previous methods mainly focus on solving the match task in single-domain setting, which may not work well when labeled data is limited. 

Learning to Infer Entities, Properties and their Relations from Clinical Conversations 
Nan Du, Mingqiu Wang, Linh Tran, Gang Lee, and Izhak Shafran 
Recently we proposed the Span Attribute Tagging (SAT) Model to infer clinical entities (e.g., symptoms) and their properties (e.g., duration). It tackles the challenge of large label space and limited training data using a hierarchical two-stage approach that identifies the span of interest in a tagging step and assigns labels to the span in a classification step. 

PRADO: Projection Attention Networks for Document Classification On-Device 
Prabhu Kaliamoorthi, Sujith Ravi, and Zornitsa Kozareva 
Recently, there has been a great interest in the development of small and accurate neural networks that run entirely on devices such as mobile phones, smart watches and IoT. This enables user privacy, consistent user experience and low latency. Although a wide range of applications have been targeted from wake word detection to short text classification, yet there are no on-device networks for long text classification. 

Counterfactual Story Reasoning and Generation 
Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and Yejin Choi 
13:30–13:48
Counterfactual reasoning requires predicting how alternative events, contrary to what actually happened, might have resulted in different outcomes. Despite being considered a necessary component of AI- complete systems, few resources have been developed for evaluating counterfactual reasoning in nar- ratives. 

Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model
Tsung-Yuan Hsu, Chi-Liang Liu, and Hung-yi Lee
Because it is not feasible to collect training data for every language, there is a growing interest in cross- lingual transfer learning. In this paper, we systematically explore zero-shot cross-lingual transfer learning on reading comprehension tasks with language representation model pre-trained on multi-lingual corpus. 

Constraint-based Learning of Phonological Processes 
Shraddha Barke, Rose Kunkel, Nadia Polikarpova, Eric Meinhardt, Eric Bakovic, and Leon Bergen 
16:30–16:48
Phonological processes are context-dependent sound changes in natural languages. We present an unsu- pervised approach to learning human-readable descriptions of phonological processes from collections of related utterances. Our approach builds upon a technique from the programming languages community called 
