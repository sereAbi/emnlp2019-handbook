\begin{bio}

%\textbf{Haibin Lin} TO BE DONE

%\textbf{Xingjian Shi} TO BE DONE 

%\textbf{Leonard Lausen} TO BE DONE 

%\textbf{Aston Zhang} TO BE DONE

\textbf{He He} is a senior research scientist at Amazon Web Services. Previously, she was a postdoctoral researcher at Stanford University. She earned her Ph.D. in Computer Science at the University of Maryland, College Park. She is interested in natural language processing and machine learning. Her research focuses on building intelligent agents that work in a changing environment and interact with people, with an emphasis on language-related problems. Specific applications include dependency parsing, simultaneous machine interpretation, and goal-oriented dialogue.


%\textbf{Sheng Zha} TO BE DONE

\textbf{Alexander Smola} is director of machine learning at Amazon Web Services. His research interest covers deep learning, scalability of algorithms, kernels methods, statistical modeling, and applications.


\end{bio}

\begin{tutorial}
  {Dive into Deep Learning for Natural Language Processing}
  {tutorial-final-001}
  {\daydateyear, \tutorialfulltime}
  {\TutLocA}

Deep learning has become the dominant approach to NLP problems, especially when applied on large scale corpora. Recent progress on unsupervised pre-training techniques such as BERT, ELMo, GPT-2, and language modeling in general, when applied on large corpora, is shown to be effective in improving a wide variety of downstream tasks. These techniques push the limits of available hardware, requiring specialized frameworks optimized for GPU, ASIC, and distributed cloud-based training.

A few complexities pose challenges to scale these models and algorithms effectively. Compared to other areas where deep learning is applied, these NLP models contain a variety of moving parts: text normalization and tokenization, word representation at subword-level and word-level, variable-length models such as RNN and attention, and sequential decoder based on beam search, among others.

In this hands-on tutorial, we take a closer look at the challenges from these complexities and see how with proper tooling with Apache MXNet and GluonNLP, we can overcome these challenges and achieve state-of-the-art results for real-world problems. GluonNLP is a powerful new toolkit that combines MXNetâ€™s speed, the flexibility of Gluon, and an extensive new library automating the most laborious aspects of deep learning for NLP.

\end{tutorial}
