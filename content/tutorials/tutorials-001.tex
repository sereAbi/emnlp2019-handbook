\begin{bio}

\textbf{Haibin Lin} TO BE DONE

\textbf{Xingjian Shi} TO BE DONE 

\textbf{Leonard Lausen} TO BE DONE 

\textbf{Aston Zhang} TO BE DONE

\textbf{He He} TO BE DONE

\textbf{Sheng Zha} TO BE DONE

\textbf{Alexander Smola} TO BE DONE

\end{bio}

\begin{tutorial}
  {Dive into Deep Learning for Natural Language Processing}
  {tutorial-final-001}
  {\daydateyear, \tutorialmorningtime}
  {\TutLocA}

Deep learning has become the dominant approach to NLP problems, especially when applied on large scale corpora. Recent progress on unsupervised pre-training techniques such as BERT, ELMo, GPT-2, and language modeling in general, when applied on large corpora, is shown to be effective in improving a wide variety of downstream tasks. These techniques push the limits of available hardware, requiring specialized frameworks optimized for GPU, ASIC, and distributed cloud-based training.

A few complexities pose challenges to scale these models and algorithms effectively. Compared to other areas where deep learning is applied, these NLP models contain a variety of moving parts: text normalization and tokenization, word representation at subword-level and word-level, variable-length models such as RNN and attention, and sequential decoder based on beam search, among others.

In this hands-on tutorial, we take a closer look at the challenges from these complexities and see how with proper tooling with Apache MXNet and GluonNLP, we can overcome these challenges and achieve state-of-the-art results for real-world problems. GluonNLP is a powerful new toolkit that combines MXNetâ€™s speed, the flexibility of Gluon, and an extensive new library automating the most laborious aspects of deep learning for NLP.

\end{tutorial}
