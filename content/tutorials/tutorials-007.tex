

\begin{tutorial}{Semantic Specialization of Distributional Word Vectors}
  {tutorial-final-007}
  {\daydateyear, \tutorialafternoontime}
  {\TutLocG}

Distributional word vectors have become an indispensable component of most state-of-art NLP models. As a major artefact of the underlying distributional hypothesis, distributional word vector spaces conflate various paradigmatic and syntagmatic lexico-semantic relations. For example, relations such as synonymy/similarity (e.g., car-automobile) or lexical entailment (e.g., car-vehicle) often cannot be distinguished from antonymy (e.g., black-white), meronymy (e.g., car-wheel) or broader thematic relatedness (e.g., car-driver) based on the distances in the distributional vector space. This inherent property of distributional spaces often harms performance in downstream applications, since different lexico-semantic relations support different classes of NLP applications. For instance, Semantic Similarity provides guidance for Paraphrasing, Dialogue State Tracking, and Text Simplification, Lexical Entailment supports Natural Language Inference and Taxonomy Induction, whereas broader thematic relatedness yields gains for Named Entity Recognition, Parsing, and Text Classification and Retrieval.

A plethora of methods have been proposed to emphasize specific lexico-semantic relations in a reshaped (i.e., specialized) vector space. A common solution is to move beyond purely unsupervised word representation learning and include external lexico-semantic knowledge, in a process commonly referred to as semantic specialization. In this tutorial, we provide a thorough overview of specialization methods, covering: 1) joint specialization methods, which augment distributional learning objectives with external linguistic constraints, 2) post-processing retrofitting models, which fine-tune pre-trained distributional vectors to better reflect external linguistic constraints, and 3) the most recently proposed post-specialization methods that generalize the perturbations of the post-processing methods to the whole distributional space. In addition to providing a comprehensive overview of specialization methods, we will introduce the most recent developments, such as (among others): handling asymmetric relations (e.g., hypernymy-hyponymy) in Euclidean and hyperbolic spaces by accounting for vector magnitude as well as for vector distance; cross-lingual transfer of semantic specialization for languages without external lexico-semantic resources; downstream effects of specializing distributional vector spaces; injecting external knowledge into unsupervised pretraining architectures such as ELMo or BERT.

\end{tutorial}

\begin{bio}

\textbf{Goran Glavaś} is an Assistant Professor of Statistical Natural Language Processing at the Data and Web Science group, School of Business Informatics and Mathematics, University of Mannheim. He obtained his Ph.D. at the Text Analysis and Knowledge Engineering Lab (TakeLab), University of Zagreb. His research efforts and interests are in the areas of statistical natural language processing (NLP) and information retrieval (IR), with focus on lexical and computational semantics, multi-lingual and cross-lingual NLP and IR, information extraction, and NLP applications for social sciences. He is a co-organizer of the TextGraphs workshop series on Graph-Based NLP. He has given an invited talk on specialization of word vector spaces at the 7th International Conference on Analysis of Images, Social Networks and Texts (AIST) and a number of invited talks at academic institutions.


\textbf{Edoardo Maria Ponti} is a final-year PhD student in the Language Technology Lab at the University of Cambridge. He holds a Master’s degree in Computational Linguistics from the Universityof Pavia, for which he was awarded honours and a recommendation for thesis publication, and a diploma on theoretical syntax and neurosciences from IUSS. His research interests include linguistic typology, syntax-based Natural Language Processing, representation learning, cross-lingual transfer, and causal inference. He has been a supervisor for the course on Computational Linguistics and demonstrator for the course on Machine Learning for Real-World Data at the University of Cambridge. He prepared the material for the course “The Lexicon: an Interdisciplinary Introduction” at ESSLLI 2018, and was recently invited to give a talk at the University of Kyoto (Japan), among others.


\textbf{Ivan Vulić} is a Senior Research Associate in the Language Technology Lab at the University of Cambridge, and a Senior Scientist at PolyAI. He holds a PhD from KU Leuven, obtained summa cum laude. Ivan is interested in representation learning, human language understanding, distributional, lexical, and multi-modal semantics in monolingual and multilingual contexts, and transfer learning for enabling cross-lingual NLP applications and bringing conversational AI to resource-poor languages. He has co-lectured a tutorial on monolingual and multilingual topic models and applications at ECIR 2013 and WSDM 2014, a tutorial on semantic specialization at EACL 2017 and ESSLLI 2018, a tutorial on cross-lingual word representations at EMNLP 2017, and several tutorials on conversational AI, including the ones at the Conversational Intelligence Summer School 2018 and NAACL 2018. He has given a large number of invited talks at academia and industry, and serves as an area chair for NAACL 2019 and ACL 2019.


\end{bio}