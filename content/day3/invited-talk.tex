%\thispagestyle{myheadings}
\section{Invited Talk: Kyunghyun Cho}
\index{Cho, Kyunghyun}

\begin{center}
\begin{Large}
    {\bfseries\Large Curiosity-driven Journey into Neural Sequence Models} \vspace{1em}\par
\end{Large}

%% \begin{center}
%%   \begin{tabular}{m{1in}b{1in}}
%%     \includegraphics[width=1in]{content/monday/cortes-headshot.png}
%%     & {\bfseries Corinna Cortes} \newline Google Research, NY
%%   \end{tabular}
%% \end{center}

\daydateyear, 09:00--10:00 \vspace{1em}\\
\PlenaryLoc \\
\vspace{1em}\par
%\includegraphics[height=100px]{content/monday/cortes-headshot.png}
\end{center}

\noindent
{\bfseries Abstract:} In this talk, I take the audience on a tour of my earlier and recent experiences in building neural sequence models. I start from the earlier experience of using a recurrent net for sequence-to-sequence learning and talk about the attention mechanism. I discuss factors behind the success of these earlier approaches, and how these were embraced by the community even before they sota'd. I then move on to more recent research direction in unconventional neural sequence models that automatically learn to decide on the order of generation.
\vspace{3em}\par 

\vfill
\noindent

{\bfseries Biography:} Kyunghyun Cho is an associate professor of computer science and data science at New York University and a research scientist at Facebook AI Research. He was a postdoctoral fellow at University of Montreal until summer 2015 under the supervision of Prof. Yoshua Bengio, and received PhD and MSc degrees from Aalto University early 2014 under the supervision of Prof. Juha Karhunen, Dr. Tapani Raiko and Dr. Alexander Ilin. He tries his best to find a balance among machine learning, natural language processing, and life, but almost always fails to do so.

\newpage
