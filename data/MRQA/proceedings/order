+ 9:00--9:35 Invited talk I: Antoine Bordes
+ 9:35--10:10 Invited talk II: Matt Gardner
= Best paper session I
44 10:10--10:30 # Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering
+ 10:30--11:00 Coffee break
+ 11:00--11:35 Invited talk III: Jordan Boyd-Graber
+ 11:35--12:10 Shared task overview
= Shared task best system session
64 12:10--12:30 # D-NET: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension
+ 12:30--14:00 Lunch break
= Best paper session II
45 14:00-14:20 # Evaluating Question Answering Evaluation
+ 14:20--14:55 Invited talk IV: Mohit Bansal
+ 14:55--15:30 Poster lightning talks
= Poster session
3 15:30--16:30 # Inspecting Unification of Encoding and Matching with Transformer: A Case Study of Machine Reading Comprehension
6 15:30--16:30 # CALOR-QUEST : generating a training corpus for Machine Reading Comprehension models from shallow semantic annotations
14 15:30--16:30 # Improving Subject-Area Question Answering with External Knowledge
18 15:30--16:30 # Answer-Supervised Question Reformulation for Enhancing Conversational Machine Comprehension
19 15:30--16:30 # Simple yet Effective Bridge Reasoning for Open-Domain Multi-Hop Question Answering
23 15:30--16:30 # Improving the Robustness of Deep Reading Comprehension Models by Leveraging Syntax Prior
25 15:30--16:30 # Reasoning Over Paragraph Effects in Situations
26 15:30--16:30 # Towards Answer-unaware Conversational Question Generation
28 15:30--16:30 # Cross-Task Knowledge Transfer for Query-Based Text Summarization
29 15:30--16:30 # Book QA: Stories of Challenges and Opportunities
30 15:30--16:30 # FlowDelta: Modeling Flow Information Gain in Reasoning for Conversational Machine Comprehension
34 15:30--16:30 # Do Multi-hop Readers Dream of Reasoning Chains?
42 15:30--16:30 # Machine Comprehension Improves Domain-Specific Japanese Predicate-Argument Structure Analysis
43 15:30--16:30 # On Making Reading Comprehension More Comprehensive
44 15:30--16:30 # Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering
45 15:30--16:30 # Evaluating Question Answering Evaluation
46 15:30--16:30 # Bend but Donâ€™t Break? Multi-Challenge Stress Test for QA Models
47 15:30--16:30 # ReQA: An Evaluation for End-to-End Answer Retrieval Models
48 15:30--16:30 # Comprehensive Multi-Dataset Evaluation of Reading Comprehension
50 15:30--16:30 # A Recurrent BERT-based Model for Question Generation
52 15:30--16:30 # Let Me Know What to Ask: Interrogative-Word-Aware Question Generation
59 15:30--16:30 # Extractive NarrativeQA with Heuristic Pre-Training
60 15:30--16:30 # CLER: Cross-task Learning with Expert Representation to Generalize Reading and Understanding
61 15:30--16:30 # Question Answering Using Hierarchical Attention on Top of BERT Features
62 15:30--16:30 # Domain-agnostic Question-Answering with Adversarial Training
63 15:30--16:30 # Generalizing Question Answering System with Pre-trained Language Model Fine-tuning
64 15:30--16:30 # D-NET: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension
65 15:30--16:30 # MRQA 2019 Shared Task: Fine-tuned XLNet with Negative Sampling for Multi-Domain Question Answering
4 15:30--16:30 # Errudite: Scalable, Reproducible, and Testable Error Analysis
17 15:30--16:30 # Are Red Roses Red? Evaluating Consistency of Question-Answering Models
32 15:30--16:30 # Revealing the Importance of Semantic Retrieval for Machine Reading at Scale
57 15:30--16:30 # Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension
+ 16:30--17:30 Panel discussion